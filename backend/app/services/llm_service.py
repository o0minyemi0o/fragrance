from groq import Groq
from app.schema.config import settings
from app.prompts import get_accord_generation_prompt, get_formula_generation_prompt
from app.db.queries import get_ingredient_names
from sqlalchemy.orm import Session
from typing import Optional
import logging
import json

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        try:
            logger.info("ğŸ”§ Groq Client ì´ˆê¸°í™” ì¤‘...")
            self.client = Groq(api_key=settings.GROQ_API_KEY)
            logger.info("âœ“ Groq Client ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"Groq Client ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def generate_accord(self, accord_type: str, db: Optional[Session] = None, use_available_ingredients: bool = False) -> dict:
        """Accord ì¡°í•© ìƒì„±"""
        # Get ingredient names from DB only if explicitly requested
        ingredient_names = None
        if use_available_ingredients and db:
            try:
                ingredient_names = get_ingredient_names(db)
                logger.info(f"Loaded {len(ingredient_names)} ingredients from DB for context")
            except Exception as e:
                logger.warning(f"Failed to load ingredients: {e}")

        prompt = get_accord_generation_prompt(accord_type, ingredient_names)

        try:
            logger.info(f"ğŸš€ Accord ìƒì„± ì‹œì‘: {accord_type}")
            response = self.client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            result_text = response.choices[0].message.content
            logger.info(f"âœ“ Accord ì‘ë‹µ ì™„ë£Œ")
            
            # JSON íŒŒì‹±
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError:
                # JSON ì¶”ì¶œ ì‹œë„
                start = result_text.find('{')
                end = result_text.rfind('}') + 1
                result = json.loads(result_text[start:end])
            
            return result
        except Exception as e:
            logger.error(f"Accord ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            raise

    def generate_formula(self, formula_type: str, db: Optional[Session] = None, use_available_ingredients: bool = False) -> dict:
        """Formula ì¡°í•© ìƒì„± (ì™„ì œí’ˆìš©, ê³ ì™„ì„±ë„)"""
        # Get ingredient names from DB only if explicitly requested
        ingredient_names = None
        if use_available_ingredients and db:
            try:
                ingredient_names = get_ingredient_names(db)
                logger.info(f"Loaded {len(ingredient_names)} ingredients from DB for context")
            except Exception as e:
                logger.warning(f"Failed to load ingredients: {e}")

        prompt = get_formula_generation_prompt(formula_type, ingredient_names)

        try:
            logger.info(f"ğŸš€ Formula ìƒì„± ì‹œì‘: {formula_type}")
            response = self.client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            result_text = response.choices[0].message.content
            logger.info(f"âœ“ Formula ì‘ë‹µ ì™„ë£Œ")
            
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError:
                start = result_text.find('{')
                end = result_text.rfind('}') + 1
                result = json.loads(result_text[start:end])
            
            return result
        except Exception as e:
            logger.error(f"Formula ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            raise

    async def stream_chat(self, messages: list, system_prompt: str):
        """ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì²« ë©”ì‹œì§€ë¡œ ì¶”ê°€
        chat_history = [{"role": "system", "content": system_prompt}]

        # ê¸°ì¡´ ë©”ì‹œì§€ ì¶”ê°€
        for msg in messages:
            chat_history.append({
                "role": msg['role'],
                "content": msg['content']
            })

        # Groq streaming API
        stream = self.client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=chat_history,
            temperature=0.7,
            stream=True
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

llm_service = LLMService()